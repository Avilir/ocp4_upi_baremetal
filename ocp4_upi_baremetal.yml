---

# you must run discover_macs.yml playbook first when
# you first get a lab reservation to obtain an inventory
# file with the MAC addresses of the deployment interface 
# in them.   All hosts must be running RHEL8 at that time.
# You only need to do this once!
# Use this output inventory file to run this playbook.

# you must have password-less ssh access to the deployer host

# set up deployer host

- hosts: all
  remote_user: root
  gather_facts: no
  tasks:
  - name: include common vars
    include_vars:
      file: common_vars.yml

  # check that group_vars/all.yml variables are defined

  - name: check all.yml vars are defined
    fail:
      msg: "bad pronoun, you did not define group_vars/all.yml vars!"
    when: 
    - lab_name is not defined
    - ipmi_password is not defined
    - rhcos_url is not defined
    - openshift_release_url is not defined
    - ocp4_pull_secret is not defined

  # look up vars using metadata based on lab and machine type

  - name: include metadata
    include_vars:
      file: "{{ lab_metadata_file }}"

  - name: extract per_lab_info from lab_infra
    set_fact: 
      per_lab_info: "{{ item }}"
    when: item.name == lab_name
    with_items: "{{ lab_metadata }}"

  - name: extract machine info from per-host machine type
    set_fact:
      machine_info: "{{ item }}"
    when: item.machine_type == machine_type
    with_items: "{{ per_lab_info.machine_types }}"

  # mac address already comes from discover_macs.yml via inventory_with_macs.yml
 
  - name: extract deploy interface, disabled interfaces and boot order
    set_fact:
      deploy_intf: "{{ machine_info.provision_intf }}"
      disabled_intfs: "{{ machine_info.disabled_intfs }}"
      badfish_boot_order: "{{ machine_info.badfish_boot_order }}"
      public_intf: "{{ machine_info.public_intf }}"



# all lookups performed, start installing!

- hosts: deployer
  remote_user: root
  vars:
    # needed for IP address assignment to masters and workers
    # in dnsmasq.conf
    master_count: "{{ groups['masters'] | length }}"
    worker_count: "{{ groups['workers'] | length }}"

  tasks:

  # construct list of machine types

  - name: remove machine type file
    local_action: "file path={{ machine_types_by_node }} state=absent"

  - name: append machine_type to this file for each host
    local_action: command sh -c "echo {{ hostvars[item].machine_type }} >> {{ machine_types_by_node }}"
    with_items: "{{ groups['all_openshift'] }}"
  
  - name: sort out duplicates to get machine type list
    local_action: command sort -u "{{ machine_types_by_node }}"
    register: site_machine_types

  - name: list all machine types for this site
    local_action: command echo "{{ item }}"
    with_items: site_machine_types.stdout_lines

  # find time server to use

  - name: for alias, use westford MA US time server
    set_fact:
      time_server: clock.bos.redhat.com
    when: lab_name == "alias"

  - name: for scale_lab, use Raleigh NC US time server
    set_fact:
      time_server: clock1.rdu2.redhat.com
    when: lab_name == "scale_lab"


  # turn off any disabled interfaces (for Q-in-Q 1 configs)

  - name: get list of active interfaces
    shell: "nmcli conn show | grep -v -- -- | grep -v NAME | grep -v {{ public_intf }} | awk '{ print $NF }'"
    register: deployer_disable_intfs

  - name: get subset in disabled_intfs list
    set_fact:
      intfs_to_disable: "{{ deployer_disable_intfs.stdout_lines | intersect(disabled_intfs) }}"

  - name: down these interfaces using uuid from NetworkManager
    shell: "nmcli conn down $(nmcli conn show | grep {{ item }} | awk '{ print $(NF-2) }')"
    with_items: "{{ intfs_to_disable }}"
    
  # so reboot won't bring them back online ...
  # note: some of them may not be online so errors can be seen here

  - name: disable these interfaces using NetworkManager
    shell: "nmcli device disconnect {{ item }}"
    ignore_errors: yes
    with_items: "{{ intfs_to_disable }}"
    

  # install RPMS
 
  - name: pull common variables
    include_vars:
       file: common_vars.yml

  - name: must be RHEL 8.*
    shell: "grep 'release 8' /etc/redhat-release"

  - name: see if packages are installed
    shell: "rpm -q virt-install"
    ignore_errors: true
    register: packages_installed

  # only do this stuff if you have never run on this deployer host before
 
  - name: upgrade to latest RHEL GA
    block:
        - debug:
            msg: "upgrading to latest RHEL GA"

        - name: ensure no lab per-VLAN interfaces defined
          shell: "bash ~/clean-interfaces.sh --nuke"

        # NOT SECURE
        #- name: register with subscription manager
        #  subscription-manager:
        #    username: "{{ environment.rh_username }}"
        #    password: "{{ environment.rh_password }}"
        #    auto_attach: yes
        #    ignore_errors: yes

        # this will fail if user hasn't done subscription-manager register
        # but if there are repos in place, that's ok.
 
        - name: attach relevant repos
          shell: "subscription-manager attach --auto"
          ignore_errors: yes

        - name: upgrade to new version 
          yum:
            update_only: yes
            state: latest
            name: '*'

        - name: install virtualization packages
          yum: 
            name: '@Virtualization Host'

        - name: install OCP4-related packages
          yum: 
            name: "ipmitool,wget,virt-install,jq,python3,podman,httpd,syslinux-tftpboot,haproxy,httpd,virt-install,vim-enhanced,git,tmux,python3-beautifulsoup4,ansible,tigervnc-server,tigervnc-server-module,firefox,python3-psutil,gnome-session,gnome-classic-session,python3-aiohttp"

        - name: ensure changes committed
          shell: sync

        - name: use permissive mode for SELINUX on deployer host
          replace:
            path: "/etc/selinux/config"
            regexp: "SELINUX=enforcing"
            replace: "SELINUX=permissive"

        - name: reboot
          reboot: 
            post_reboot_delay: 10
            pre_reboot_delay: 1

    when: packages_installed.rc != ok


  - name: put SELINUX in permissive mode
    shell: "setenforce 0"
    ignore_errors: true

  - name: see if vnc running
    pids:
      name: Xvnc
    register: vnc_pids

  - name: show vnc_pids length
    local_action: command echo "{{ vnc_pids.pids | length }}"

  - name: setup VNC
    when: (vnc_pids.pids | length) == 0
    block:
    - name: create vnc directory
      file:
        path: .vnc
        state: directory

    - name: set vnc passwd
      shell: "echo {{ vncpasswd }} | vncpasswd -f > ~/.vnc/passwd"

    - name: make it only readable by root
      file:
        path: ~/.vnc/passwd
        mode: 0600

    - name: change user to root in unit file
      replace:
        path: /usr/lib/systemd/system/vncserver@.service
        regexp: "<USER>"
        replace: "root"

    - name: ensure vncserver stays started
      systemd:
          enabled: yes
          name: vncserver@:1
          state: started


  # set up TFTP boot config

  - name: see if tftpboot already set up
    shell: "ls -l {{ pxedir }}/bootstrap.type_vm"
    ignore_errors: yes
    register: tftp_exists

  - name: set up tftpboot
    when: tftp_exists.rc != ok
    block:

    - name: check we have deploy_mac defined
      fail:
        msg: "deploy_mac not defined for host {{ item }}, should be in inventory file"
      when: "{{ hostvars[item]['deploy_mac'] is not defined }}"
      with_items: "{{ groups['all_openshift'] }}"

    - name: make PXE boot directory world-readable
      file:
        path: "{{ pxedir }}"
        state: directory
        mode: go+rx

    - name: make parent directory world-readable
      file:
        path: "{{ pxedir }}/.."
        state: directory
        mode: go+rx

    - name: copy boot files into TFTP parent dir
      shell: 
        chdir: "{{ pxedir }}"
        cmd: "cp -v /tftpboot/lpxelinux.0 /tftpboot/ldlinux.c32 ../ && chmod a+rx ../*linux*"

    - name: generate per-machine-type boot file for masters and workers
      template: 
        src: "pxelinux-cfg-default.j2"
        dest: "{{ pxedir }}/{{ item[0] }}.type_{{ item[1] }}"
        force: yes
      loop: "{{ ['masters','workers'] | product(site_machine_types.stdout_lines) | list }}"

    - name: for master, softlink bootstrap mac addr to PXE for machine type and role
      file:
        state: link
        force: yes
        src: "{{ pxedir }}/masters.type_{{ hostvars[item]['machine_type'] }}"
        path: "{{ pxedir }}/01-{{ hostvars[item]['deploy_mac'] | replace(':','-') }}"
      loop: "{{ groups['masters'] }}"

    - name: for worker, softlink bootstrap mac addr to PXE for machine type and role
      file:
        state: link
        force: yes
        src: "{{ pxedir }}/workers.type_{{ hostvars[item]['machine_type'] }}"
        path: "{{ pxedir }}/01-{{ hostvars[item]['deploy_mac'] | replace(':','-') }}"
      loop: "{{ groups['workers'] }}"

    # treat bootstrap special, there is only 1 of them ever
    # use machine type "vm", someday that may be a real thing
 
    - name: generate bootstrap VM boot file
      template:
        src: "pxelinux-cfg-default.j2"
        dest: "{{ pxedir }}/bootstrap.type_vm"
      loop: "{{ ['bootstrap'] | product(['vm']) | list }}"

    - name: softlink bootstrap mac addr to PXE for bootstrap
      file:
        state: link
        force: yes
        src: "{{ pxedir }}/bootstrap.type_vm"
        path: "{{ pxedir }}/01-{{ bootstrap_mac | replace(':','-') }}"

    - name: make SELinux happy
      shell: "restorecon -r {{ pxedir }}"


  # set up dnsmasq config

  - name: see if dnsmasq.conf already edited
    shell: "grep -q bootstrap {{ dnsmasq_conf }}"
    ignore_errors: yes
    register: dnsmasq_edited

  - name: set up dnsmasq
    when: dnsmasq_edited.rc != ok
    block:
    - name: save original dnsmasq.conf 
      shell: "if [ ! {{ dnsmasq_conf }}.orig ] ; then cp {{ dnsmasq_conf }} {{ dnsmasq_conf }}.orig ; fi"

    - name: copy dnsmasq OCP4-specific template
      template:
        src: "ocp4-upi-dnsmasq.conf.j2"
        dest: "/var/tmp/ocp4-upi-dnsmasq.conf"
        force: yes
        mode: go+rx

    - name: append it to dnsmasq config
      shell: "cat /var/tmp/ocp4-upi-dnsmasq.conf >> {{ dnsmasq_conf }}"


  # make sure resolv.conf stays the way we want it
 
  - name: see if NetworkManager knows not to overwrite resolv.conf
    shell: "grep -q dns=none {{ netmgr_conf }}"
    ignore_errors: yes
    register: nmconf_edited
    
  - name: disable NetworkManager overwriting resolv.conf
    when: nmconf_edited.rc != ok
    block:
      - name: edit NetworkManager conf file
        lineinfile:
          path: /etc/NetworkManager/NetworkManager.conf
          line: "dns=none"
          insertafter: '\[main\]'

      - name: restart NetworkManager so it sees the change
        systemd:
          enabled: yes
          name: NetworkManager
          state: restarted


  - name: prevent dhclient from forgetting our resolv.conf
    block:
    - name: insert our dnsmasq as a name server
      lineinfile:
        path: "{{ dhclient_conf }}"
        line: "prepend domain-name-servers {{ deployer_ip }};"

    - name: add DNS domain name to search list
      lineinfile:
        path: "{{ dhclient_conf }}"
        line: 'supercede domain-search "test.myocp4.com", "alias.bos.scalelab.redhat.com";'


    # chrony for masters and workers must be set up post-install
 
  - name: see if timeserver is available
    shell: "grep -q {{ time_server }} {{ chrony_conf }}"
    ignore_errors: yes
    register: chrony
    
  - name: make deployer host be timeserver also
    when: chrony.rc != ok
    block:
    - name: remove default time servers
      lineinfile:
        path: "{{ chrony_conf }}"
        state: absent
        regexp: "^pool"

    - name: add our time server
      lineinfile:
        path: "{{ chrony_conf }}"
        state: present
        line: "server {{ time_server }}"
        insertbefore: BOF

    - name: define subnet that we use for time services
      lineinfile:
        path: "{{ chrony_conf }}"
        line: "allow {{ deploy_subnet }}"

    - name: enable logging
      replace:
        path: "{{ chrony_conf }}"
        regexp: "#log "
        replace: "log "

    - name: restart chronyd service
      systemd:
        enabled: yes
        name: chronyd
        state: restarted



  # set up haproxy config
 
  - name: see if haproxy.conf already edited
    shell: "grep -q :6443 {{ haproxy_cfg }}"
    ignore_errors: yes
    register: haproxy_edited

  - name: set up haproxy
    when: haproxy_edited.rc != ok
    block:
    - name: generate haproxy entries for masters and workers
      template:
        src: "ocp4-upi-haproxy.cfg.j2"
        dest: "/var/tmp/ocp4-upi-haproxy.cfg"

    - name: save original haproxy.conf
      shell: "cp -v {{ haproxy_cfg }} {{ haproxy_cfg }}.orig"

    - name: append to haproxy config
      shell: "cat /var/tmp/ocp4-upi-haproxy.cfg >> {{ haproxy_cfg }}"


  # pull RHCOS and make it available for PXE booters

  - name: see if RHCOS is ready
    shell: 
      cmd: "cd {{ webdir }} && ls -lL rhcos-metal-bios.raw.gz rhcos-installer-initramfs.img rhcos-installer-kernel"
    ignore_errors: yes
    register: rhcos_downloaded

  - name: setup RHCOS for boot
    when: rhcos_downloaded.rc != ok
    block:
    - name: configure httpd
      lineinfile:
        path: /etc/httpd/conf/httpd.conf
        firstmatch: yes
        line: "Listen 81"
        regexp: "Listen 80"

    - name: create subdir for HTTP-served files
      file:
        mode: go+rx
        path: "{{ webdir }}"
        state: directory

    - name: clean subdir for RHCOS tarball index
      file:
        path: rhcos
        state: absent

    - name: make new subdir for RHCOS tarball index
      file:
        path: rhcos
        state: directory

    - name: pull index.html from URL
      shell: 
        chdir: rhcos
        cmd: "wget {{ rhcos_url }}"

    - name: install program to parse contents of web URL
      copy:
        src: parse_web_dir.py
        dest: "/usr/local/bin"
        mode: go+rx

    - name: extract relevant sub-URLs from index.html
      shell:
        chdir: rhcos
        cmd: "parse_web_dir.py index.html rhcos installer-kernel,installer-initramfs,metal"
      register: suburls

    - name: download just these sub-URLs
      shell:
        chdir: "{{ webdir }}"
        cmd: "wget -nv {{ rhcos_url }}/{{ item }}"
      with_items: "{{ suburls.stdout_lines }}"

    - name: create kernel softlink
      shell: 
        cmd: "ln -sv *installer-kernel* rhcos-installer-kernel"
        chdir: "{{ webdir }}"

    - name: create initramfs softlink
      shell: 
        cmd: "ln -sv *installer-initramfs* rhcos-installer-initramfs.img"
        chdir: "{{ webdir }}"

    - name: create bios softlink
      shell: 
        cmd: "ln -sv *metal* rhcos-metal-bios.raw.gz"
        chdir: "{{ webdir }}"

    - name: check a softlink
      shell: 
        cmd: "ls -lL rhcos-metal-bios.raw.gz"
        chdir: "{{ webdir }}"


  # turn on arp filtering

  - name: create arp filter config file
    shell: "echo net.ipv4.conf.all.arp_filter = 1 > /etc/sysctl.d/arp.conf"

  - name: and run it
    shell: "sysctl -p /etc/sysctl.d/arp.conf"


  # set up network interface for deployment

  - name: see if bridge already set up
    shell: "ip a | grep {{ deploy_bridge }}"
    ignore_errors: yes
    register: bridge_exists

  - name: ping host IP
    shell: "ping -c 2 {{ deployer_ip }}"
    ignore_errors: yes
    register: ping_deployer

  - name: set up network bridge
    when: bridge_exists.rc != ok or ping_deployer.rc != ok
    block:
        - name: take down interface
          shell: "nmcli con down {{ deploy_intf }}"
          ignore_errors: yes

        - name: delete interface
          shell: "nmcli con del {{ deploy_intf }}"
          ignore_errors: yes

        - name: add bridge
          shell: "nmcli con add type bridge ifname {{ deploy_bridge }} con-name {{ deploy_bridge }} ipv4.method manual ipv4.addr {{ deployer_ip }}/{{ deploy_ip_prefix }} autoconnect yes bridge.stp no"

        - name: add interface as bridge slave
          shell: "nmcli con add type bridge-slave autoconnect yes con-name {{ deploy_intf }} ifname {{ deploy_intf }} master {{ deploy_bridge }}"

        - name: define bridge
          shell: "nmcli con reload {{ deploy_intf }}"

        - name: define interface
          shell: "nmcli con reload {{ deploy_bridge }}"

        - name: bring bridge online
          shell: "nmcli con up {{ deploy_bridge }}"

        - name: bring interface online
          shell: "nmcli con up {{ deploy_intf }}"

        - name: wait a few sec...
          shell: "sleep 2"

        - name: ping host IP
          shell: "ping -c 2 {{ deployer_ip }}"


  # this is necessary to ensure libvirtd doesn't have dnsmasq running
 
  - name: undefine default network
    shell: "virsh net-destroy {{ item }}"
    ignore_errors: yes
    with_items:
    - default
    - ocp4-upi


  - name: shut down libvirtd (and its dnsmasq)
    systemd:
      enabled: yes
      name: libvirtd
      state: stopped

  - name: kill any remaining dnsmasq processes
    shell: "killall dnsmasq"
    ignore_errors: yes

  - name: edit resolv.conf in an idempotent way
    block:
    - name: add comment saying modified by this playbook
      lineinfile:
        path: "{{ resolv_conf }}"
        line: "# modified by ocp4_upi_baremetal.yml"
        insertbefore: BOF

    - name: remove cluster domain name from search list
      replace:
        path: "{{ resolv_conf }}"
        regexp: "(.*) {{ ocp4_dns_domain_name }}(.*)"
        replace: '\1\2'

    - name: add cluster name to resolv.conf search list
      replace:
        path: "{{ resolv_conf }}"
        regexp: "(^search)(.*)"
        replace: '\1 {{ ocp4_dns_domain_name }}\2'

    - name: remove deployer DNSmasq server from resolv.conf
      lineinfile:
        path: "{{ resolv_conf }}"
        line: "nameserver {{ deployer_ip }}"
        state: absent
      
    - name: add DNS server IP addr to resolv.conf
      lineinfile:
        path: "{{ resolv_conf }}"
        line: "nameserver {{ deployer_ip }}"
        insertbefore: "^nameserver.*"
        state: present

  - name: flush iptables
    shell: "iptables -F"

  - name: configure NAT in iptables
    shell: "iptables -t nat -A POSTROUTING -s {{ deploy_subnet }} ! -d {{ deploy_subnet }} -o {{ public_intf }} -j MASQUERADE"

  - name: save iptables config
    shell: "iptables-save > /etc/sysconfig/iptables"

  - name: bring up services
    systemd:
      enabled: yes
      name: "{{ item }}"
      state: restarted
    with_items: 
    - httpd
    - haproxy
    - dnsmasq

  - name: wait a few sec...
    shell: "sleep 2"

  - name: start up libvirtd
    systemd:
      enabled: yes
      name: libvirtd
      state: started


  # install openshift

  - name: see if openshift client and install utilities are installed
    shell: "which oc && which kubectl && which openshift-install"
    ignore_errors: yes
    register: ocp_installed

  - name: install openshift client and installer
    when: ocp_installed.rc != ok
    block:
    - name: remove any openshift tarballs
      file: 
        path: openshift
        state: absent
      
    - name: create a separate directory for openshift tarballs
      file: 
        path: openshift
        state: directory
      
    - name: pull each tarball
      shell:
        chdir: openshift
        cmd: "wget -nv {{ openshift_release_url }}/{{ item }}"
      with_items: 
      - openshift-client-linux
      - openshift-install-linux

    - name: unzip each tarball so it is in PATH
      shell:
        chdir: "/usr/local/bin"
        cmd: "tar zxvf ~/openshift/{{ item }}tar.gz"
      with_items: 
      - openshift-client-linux
      - openshift-install-linux

    - name: verify everything got unpacked
      shell: "which {{ item }}"
      with_items: 
      - kubectl
      - oc
      - openshift-install

    - name: report openshift version installed
      shell: "openshift-install version"



  # we don't actually PXE boot masters & workers, just get ready to

  - name: see if badfish is ready
    shell: "ls masters.list && ls badfish"
    ignore_errors: yes
    register: badfish_ready

  - name: prepare to install masters and workers
    when: badfish_ready.rc != ok
    block:

    - name: install QUADS_TICKET environment variable
      lineinfile:
        path: .bashrc
        state: present
        line: "export QUADS_TICKET={{ ipmi_password }}"

    - name: clone badfish repo
      git:
        dest: badfish
        repo: https://github.com/redhat-performance/badfish
        force: yes

    - name: remove director records from idrac_interfaces.yml
      lineinfile: 
        path: "{{ idrac_interfaces }}"
        regex: "^director_.*"
        state: absent

    - name: append badfish boot order records to idrac_interfaces.yml for each master
      lineinfile:
        path: "{{ idrac_interfaces }}"
        state: present
        line: "{{ hostvars[item].badfish_boot_order }}"
      with_items: "{{ groups['masters'] }}"
        
    - name: append badfish boot order records to idrac_interfaces.yml for each worker
      lineinfile:
        path: "{{ idrac_interfaces }}"
        state: present
        line: "{{ hostvars[item].badfish_boot_order }}"
      with_items: "{{ groups['workers'] }}"

    - name: filter out duplicate records
      shell: "sort -u {{ idrac_interfaces }} | grep -v '^#' | grep -v -- '---' > /tmp/i.yml && cp -v /tmp/i.yml {{ idrac_interfaces }}"

    - name: delete masters.list and workers.list
      file:
        path: "{{ item }}"
        state: absent
      with_items:
      - "masters.list"
      - "workers.list"

    - name: create masters.list and workers.list
      file:
        path: "{{ item }}"
        state: touch
      with_items:
      - "masters.list"
      - "workers.list"

    - name: insert masters' idrac hostnames into masters.list
      shell: "echo mgmt-{{ item }} >> masters.list"
      with_items: "{{ groups['masters'] }}"
  
    - name: insert workers' idrac hostnames into workers .list, could be empty
      shell: "echo mgmt-{{ item }} >> workers.list"
      with_items: "{{ groups['workers'] }}"

    - name: concatenate workers and masters into all_openshift.list
      shell: "cat masters.list workers.list > all_openshift.list"

    # SOMEDAY: automate badfish commands to install masters


  # post-install inventory file allows playbooks
  # to access openshift nodes after they have joined the openshift cluster

  - name: generate post-install inventory file
    template: 
      src: post_install_inventory.yml.j2
      dest: "{{ post_install_inv }}"

  - name: fetch post install inventory file to protect from deployer reinstall
    fetch:
      flat: yes
      src: "{{ post_install_inv }}"
      dest: "~/"

  - name: copy post-install playbook files to deployer host
    copy:
      src: "{{ item }}"
      dest: "~/"
      force: yes
    with_items:
    - "post_install.yml"
    - "~/post_install_inventory.yml"
    - "common_vars.yml"
    - "group_vars/all.yml"


  # always set up openshift install ignition files
  # because they expire after 24 hours!

  - name: generate ignition files
    block:
    - name: generate ssh keypair on deployer
      openssh_keypair: 
        path: ".ssh/id_rsa"
        type: "rsa"

    - name: get public key from deployer
      shell: "cat .ssh/id_rsa.pub"
      register: deployer_public_key

    - name: make this available to template
      set_fact:
        ocp4_public_key: "{{ deployer_public_key.stdout_lines[0] }}"

    - name: add this to deployer's authorized keys for post_install.yml
      authorized_key:
        key: "{{ ocp4_public_key }}"
        user: root

    - name: get transpiler
      git:
        dest: filetranspiler
        repo: https://github.com/ashcrow/filetranspiler

    - name: clear out ignition directory
      file:
        state: absent
        path: ignition
      
    - name: create ignition directory
      file:
        state: directory
        path: ignition
      
    - name: generate ignition config yaml
      template:
        src: ocp4-upi-install-config.yaml.j2
        dest: ignition/install-config.yaml

    - name: generate manifests
      shell: "openshift-install create manifests --dir=ignition"

    - name: create ignition files
      shell: "openshift-install create ignition-configs --dir=ignition"

    - name: install python helper program 
      copy:
        src: gen_ign_disabled_intfs.py
        dest: /usr/local/bin/
        mode: 0755

    - name: install lab metadata
      copy:
        src: lab_metadata.yml
        dest: "~/"

    - name: generate per-machine-type disabled interface tree for ignition
      shell: "gen_ign_disabled_intfs.py ~/lab_metadata.yml ignition {{ lab_name }}"

    - name: run transpiler to create modified ignition files
      shell:
        chdir: "ignition"
        cmd: "cp -v {{ item[0] }}.ign {{ item[0] }}.ign.orig && python3 ~/filetranspiler/filetranspile --pretty -i {{ item[0] }}.ign.orig -f ./{{ item[1] }} -o {{ item[0] }}s.type_{{ item[1] }}.ign"
      loop: "{{ ['master','worker'] | product(site_machine_types.stdout_lines) | list }}"

    - name: remove original master.ign and worker.ign
      file:
        path: "ignition/{{ item }}.ign"
        state: absent
      with_items:
      - master
      - worker

    - name: rename bootstrap.ign to bootstrap.type_vm.ign so PXE can find it
      shell:
        chdir: ignition
        cmd: "mv bootstrap.ign bootstrap.type_vm.ign"
        
    - name: copy ignition files to where bootstrap masters and workers can get them
      shell: "cp -vf ignition/*.ign {{ webdir }}"

    - name: ignition files should be world readable
      shell: "chmod go+r {{ webdir }}/*.ign"

    - name: remember that we have new ignition files
      set_fact:
        ignition_files_recreated: yes

    - name: add KUBECONFIG variable to .bashrc
      lineinfile:
        path: .bashrc
        line: "export KUBECONFIG=~/ignition/auth/kubeconfig"
        state: present


  # once we re-generate ignition files we have to recreate bootstrap VM
 
  - name: see if bootstrap VM is running
    shell: "virsh dumpxml ocp4-upi-bootstrap | grep -i running"
    ignore_errors: yes
    register: bootstrap_started

  - name: bring up bootstrap vm
    block:
    - name: generate network XML
      template:
        src: "ocp4-upi-net.xml.j2"
        dest: "/var/tmp/ocp4-upi-net.xml"

    - name: wait a few sec...
      shell: "sleep 3"

    - name: destroy bootstrap VM
      shell: "virsh destroy {{ bootstrap_vm }} > /tmp/destroy"
      ignore_errors: yes

    - name: destroy its network 
      shell: "virsh net-destroy {{ bootstrap_net }} > /tmp/destroy-net"
      ignore_errors: yes

    - name: undefine VM
      shell: "virsh undefine {{ bootstrap_vm }} > /tmp/undefine"
      ignore_errors: yes

    - name: undefine net
      shell: "virsh net-undefine {{ bootstrap_net }} > /tmp/undefine-net"
      ignore_errors: yes

    - name: remove preexisting VM images
      shell: "rm -fv /var/lib/libvirt/images/ocp4-upi-bootstrap*"

    - name: define VM network
      shell: "virsh net-define /var/tmp/ocp4-upi-net.xml"

    - name: ensure VM network auto-starts
      shell: "virsh net-autostart {{ bootstrap_net }}"

    - name: start VM network
      shell: "virsh net-start {{ bootstrap_net }}"

    - name: wait a few sec...
      shell: "sleep 3"

    - name: install VM
      shell: "virt-install -n ocp4-upi-bootstrap --pxe --os-type=Linux --os-variant=rhel8.0 --ram=8192 --vcpus=4 --network network=ocp4-upi,mac={{ bootstrap_mac }} --disk size=120,bus=scsi,sparse=yes --check disk_size=off  --noautoconsole --wait -1"
 
    # this will block until the VM reboots, at which point it should become active

    - name: wait a few sec...
      shell: "sleep 3"

    - name: inspect VM
      shell: "virsh dumpxml ocp4-upi-bootstrap"


